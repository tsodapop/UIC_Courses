{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demographics = pd.read_csv('/Users/sakina/Desktop/project_02/demographics_test.csv')\n",
    "# merged_train = pd.read_csv('/Users/sakina/Desktop/project_02/merged_train.csv')\n",
    "\n",
    "demographics = pd.read_csv('demographics_test.csv')\n",
    "merged_train = pd.read_csv('merged_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>County</th>\n",
       "      <th>FIPS</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Percent White, not Hispanic or Latino</th>\n",
       "      <th>Percent Black, not Hispanic or Latino</th>\n",
       "      <th>Percent Hispanic or Latino</th>\n",
       "      <th>Percent Foreign Born</th>\n",
       "      <th>Percent Female</th>\n",
       "      <th>Percent Age 29 and Under</th>\n",
       "      <th>Percent Age 65 and Older</th>\n",
       "      <th>Median Household Income</th>\n",
       "      <th>Percent Unemployed</th>\n",
       "      <th>Percent Less than High School Degree</th>\n",
       "      <th>Percent Less than Bachelor's Degree</th>\n",
       "      <th>Percent Rural</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NV</td>\n",
       "      <td>eureka</td>\n",
       "      <td>32011</td>\n",
       "      <td>1730</td>\n",
       "      <td>98.265896</td>\n",
       "      <td>0.057803</td>\n",
       "      <td>0.462428</td>\n",
       "      <td>0.346821</td>\n",
       "      <td>51.156069</td>\n",
       "      <td>27.109827</td>\n",
       "      <td>15.606936</td>\n",
       "      <td>70000</td>\n",
       "      <td>3.755365</td>\n",
       "      <td>8.415466</td>\n",
       "      <td>83.396513</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TX</td>\n",
       "      <td>zavala</td>\n",
       "      <td>48507</td>\n",
       "      <td>12107</td>\n",
       "      <td>5.798299</td>\n",
       "      <td>0.594697</td>\n",
       "      <td>93.326175</td>\n",
       "      <td>9.193029</td>\n",
       "      <td>49.723301</td>\n",
       "      <td>49.302057</td>\n",
       "      <td>12.480383</td>\n",
       "      <td>26639</td>\n",
       "      <td>11.955168</td>\n",
       "      <td>40.840797</td>\n",
       "      <td>90.869691</td>\n",
       "      <td>38.032029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VA</td>\n",
       "      <td>king george</td>\n",
       "      <td>51099</td>\n",
       "      <td>25260</td>\n",
       "      <td>73.804434</td>\n",
       "      <td>16.722090</td>\n",
       "      <td>4.441805</td>\n",
       "      <td>2.505938</td>\n",
       "      <td>50.166271</td>\n",
       "      <td>40.186065</td>\n",
       "      <td>11.868567</td>\n",
       "      <td>84342</td>\n",
       "      <td>6.479939</td>\n",
       "      <td>7.152824</td>\n",
       "      <td>65.540254</td>\n",
       "      <td>73.189450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OH</td>\n",
       "      <td>hamilton</td>\n",
       "      <td>39061</td>\n",
       "      <td>805965</td>\n",
       "      <td>66.354867</td>\n",
       "      <td>25.654340</td>\n",
       "      <td>2.890944</td>\n",
       "      <td>5.086945</td>\n",
       "      <td>51.870615</td>\n",
       "      <td>40.779686</td>\n",
       "      <td>14.161657</td>\n",
       "      <td>50399</td>\n",
       "      <td>7.864630</td>\n",
       "      <td>9.873275</td>\n",
       "      <td>64.404446</td>\n",
       "      <td>2.231877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TX</td>\n",
       "      <td>austin</td>\n",
       "      <td>48015</td>\n",
       "      <td>29107</td>\n",
       "      <td>63.809393</td>\n",
       "      <td>8.479060</td>\n",
       "      <td>25.502456</td>\n",
       "      <td>9.946061</td>\n",
       "      <td>50.671660</td>\n",
       "      <td>37.351840</td>\n",
       "      <td>17.799842</td>\n",
       "      <td>56681</td>\n",
       "      <td>5.782337</td>\n",
       "      <td>17.579456</td>\n",
       "      <td>79.008391</td>\n",
       "      <td>66.344090</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  State       County   FIPS  Total Population  \\\n",
       "0    NV       eureka  32011              1730   \n",
       "1    TX       zavala  48507             12107   \n",
       "2    VA  king george  51099             25260   \n",
       "3    OH     hamilton  39061            805965   \n",
       "4    TX       austin  48015             29107   \n",
       "\n",
       "   Percent White, not Hispanic or Latino  \\\n",
       "0                              98.265896   \n",
       "1                               5.798299   \n",
       "2                              73.804434   \n",
       "3                              66.354867   \n",
       "4                              63.809393   \n",
       "\n",
       "   Percent Black, not Hispanic or Latino  Percent Hispanic or Latino  \\\n",
       "0                               0.057803                    0.462428   \n",
       "1                               0.594697                   93.326175   \n",
       "2                              16.722090                    4.441805   \n",
       "3                              25.654340                    2.890944   \n",
       "4                               8.479060                   25.502456   \n",
       "\n",
       "   Percent Foreign Born  Percent Female  Percent Age 29 and Under  \\\n",
       "0              0.346821       51.156069                 27.109827   \n",
       "1              9.193029       49.723301                 49.302057   \n",
       "2              2.505938       50.166271                 40.186065   \n",
       "3              5.086945       51.870615                 40.779686   \n",
       "4              9.946061       50.671660                 37.351840   \n",
       "\n",
       "   Percent Age 65 and Older  Median Household Income  Percent Unemployed  \\\n",
       "0                 15.606936                    70000            3.755365   \n",
       "1                 12.480383                    26639           11.955168   \n",
       "2                 11.868567                    84342            6.479939   \n",
       "3                 14.161657                    50399            7.864630   \n",
       "4                 17.799842                    56681            5.782337   \n",
       "\n",
       "   Percent Less than High School Degree  Percent Less than Bachelor's Degree  \\\n",
       "0                              8.415466                            83.396513   \n",
       "1                             40.840797                            90.869691   \n",
       "2                              7.152824                            65.540254   \n",
       "3                              9.873275                            64.404446   \n",
       "4                             17.579456                            79.008391   \n",
       "\n",
       "   Percent Rural  \n",
       "0     100.000000  \n",
       "1      38.032029  \n",
       "2      73.189450  \n",
       "3       2.231877  \n",
       "4      66.344090  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>County</th>\n",
       "      <th>FIPS</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Percent White, not Hispanic or Latino</th>\n",
       "      <th>Percent Black, not Hispanic or Latino</th>\n",
       "      <th>Percent Hispanic or Latino</th>\n",
       "      <th>Percent Foreign Born</th>\n",
       "      <th>Percent Female</th>\n",
       "      <th>Percent Age 29 and Under</th>\n",
       "      <th>Percent Age 65 and Older</th>\n",
       "      <th>Median Household Income</th>\n",
       "      <th>Percent Unemployed</th>\n",
       "      <th>Percent Less than High School Degree</th>\n",
       "      <th>Percent Less than Bachelor's Degree</th>\n",
       "      <th>Percent Rural</th>\n",
       "      <th>Democratic</th>\n",
       "      <th>Republican</th>\n",
       "      <th>Party</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AZ</td>\n",
       "      <td>apache</td>\n",
       "      <td>4001</td>\n",
       "      <td>72346</td>\n",
       "      <td>18.571863</td>\n",
       "      <td>0.486551</td>\n",
       "      <td>5.947806</td>\n",
       "      <td>1.719515</td>\n",
       "      <td>50.598513</td>\n",
       "      <td>45.854643</td>\n",
       "      <td>13.322091</td>\n",
       "      <td>32460</td>\n",
       "      <td>15.807433</td>\n",
       "      <td>21.758252</td>\n",
       "      <td>88.941063</td>\n",
       "      <td>74.061076</td>\n",
       "      <td>16298</td>\n",
       "      <td>7810</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AZ</td>\n",
       "      <td>cochise</td>\n",
       "      <td>4003</td>\n",
       "      <td>128177</td>\n",
       "      <td>56.299492</td>\n",
       "      <td>3.714395</td>\n",
       "      <td>34.403208</td>\n",
       "      <td>11.458374</td>\n",
       "      <td>49.069646</td>\n",
       "      <td>37.902276</td>\n",
       "      <td>19.756275</td>\n",
       "      <td>45383</td>\n",
       "      <td>8.567108</td>\n",
       "      <td>13.409171</td>\n",
       "      <td>76.837055</td>\n",
       "      <td>36.301067</td>\n",
       "      <td>17383</td>\n",
       "      <td>26929</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AZ</td>\n",
       "      <td>coconino</td>\n",
       "      <td>4005</td>\n",
       "      <td>138064</td>\n",
       "      <td>54.619597</td>\n",
       "      <td>1.342855</td>\n",
       "      <td>13.711033</td>\n",
       "      <td>4.825298</td>\n",
       "      <td>50.581614</td>\n",
       "      <td>48.946141</td>\n",
       "      <td>10.873943</td>\n",
       "      <td>51106</td>\n",
       "      <td>8.238305</td>\n",
       "      <td>11.085381</td>\n",
       "      <td>65.791439</td>\n",
       "      <td>31.466066</td>\n",
       "      <td>34240</td>\n",
       "      <td>19249</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AZ</td>\n",
       "      <td>gila</td>\n",
       "      <td>4007</td>\n",
       "      <td>53179</td>\n",
       "      <td>63.222325</td>\n",
       "      <td>0.552850</td>\n",
       "      <td>18.548675</td>\n",
       "      <td>4.249798</td>\n",
       "      <td>50.296170</td>\n",
       "      <td>32.238290</td>\n",
       "      <td>26.397638</td>\n",
       "      <td>40593</td>\n",
       "      <td>12.129932</td>\n",
       "      <td>15.729958</td>\n",
       "      <td>82.262624</td>\n",
       "      <td>41.062000</td>\n",
       "      <td>7643</td>\n",
       "      <td>12180</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AZ</td>\n",
       "      <td>graham</td>\n",
       "      <td>4009</td>\n",
       "      <td>37529</td>\n",
       "      <td>51.461536</td>\n",
       "      <td>1.811932</td>\n",
       "      <td>32.097844</td>\n",
       "      <td>4.385942</td>\n",
       "      <td>46.313518</td>\n",
       "      <td>46.393456</td>\n",
       "      <td>12.315809</td>\n",
       "      <td>47422</td>\n",
       "      <td>14.424104</td>\n",
       "      <td>14.580797</td>\n",
       "      <td>86.675944</td>\n",
       "      <td>46.437399</td>\n",
       "      <td>3368</td>\n",
       "      <td>6870</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  State    County  FIPS  Total Population  \\\n",
       "0    AZ    apache  4001             72346   \n",
       "1    AZ   cochise  4003            128177   \n",
       "2    AZ  coconino  4005            138064   \n",
       "3    AZ      gila  4007             53179   \n",
       "4    AZ    graham  4009             37529   \n",
       "\n",
       "   Percent White, not Hispanic or Latino  \\\n",
       "0                              18.571863   \n",
       "1                              56.299492   \n",
       "2                              54.619597   \n",
       "3                              63.222325   \n",
       "4                              51.461536   \n",
       "\n",
       "   Percent Black, not Hispanic or Latino  Percent Hispanic or Latino  \\\n",
       "0                               0.486551                    5.947806   \n",
       "1                               3.714395                   34.403208   \n",
       "2                               1.342855                   13.711033   \n",
       "3                               0.552850                   18.548675   \n",
       "4                               1.811932                   32.097844   \n",
       "\n",
       "   Percent Foreign Born  Percent Female  Percent Age 29 and Under  \\\n",
       "0              1.719515       50.598513                 45.854643   \n",
       "1             11.458374       49.069646                 37.902276   \n",
       "2              4.825298       50.581614                 48.946141   \n",
       "3              4.249798       50.296170                 32.238290   \n",
       "4              4.385942       46.313518                 46.393456   \n",
       "\n",
       "   Percent Age 65 and Older  Median Household Income  Percent Unemployed  \\\n",
       "0                 13.322091                    32460           15.807433   \n",
       "1                 19.756275                    45383            8.567108   \n",
       "2                 10.873943                    51106            8.238305   \n",
       "3                 26.397638                    40593           12.129932   \n",
       "4                 12.315809                    47422           14.424104   \n",
       "\n",
       "   Percent Less than High School Degree  Percent Less than Bachelor's Degree  \\\n",
       "0                             21.758252                            88.941063   \n",
       "1                             13.409171                            76.837055   \n",
       "2                             11.085381                            65.791439   \n",
       "3                             15.729958                            82.262624   \n",
       "4                             14.580797                            86.675944   \n",
       "\n",
       "   Percent Rural  Democratic  Republican  Party  \n",
       "0      74.061076       16298        7810      1  \n",
       "1      36.301067       17383       26929      0  \n",
       "2      31.466066       34240       19249      1  \n",
       "3      41.062000        7643       12180      0  \n",
       "4      46.437399        3368        6870      0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1195 entries, 0 to 1194\n",
      "Data columns (total 19 columns):\n",
      "State                                    1195 non-null object\n",
      "County                                   1195 non-null object\n",
      "FIPS                                     1195 non-null int64\n",
      "Total Population                         1195 non-null int64\n",
      "Percent White, not Hispanic or Latino    1195 non-null float64\n",
      "Percent Black, not Hispanic or Latino    1195 non-null float64\n",
      "Percent Hispanic or Latino               1195 non-null float64\n",
      "Percent Foreign Born                     1195 non-null float64\n",
      "Percent Female                           1195 non-null float64\n",
      "Percent Age 29 and Under                 1195 non-null float64\n",
      "Percent Age 65 and Older                 1195 non-null float64\n",
      "Median Household Income                  1195 non-null int64\n",
      "Percent Unemployed                       1195 non-null float64\n",
      "Percent Less than High School Degree     1195 non-null float64\n",
      "Percent Less than Bachelor's Degree      1195 non-null float64\n",
      "Percent Rural                            1195 non-null float64\n",
      "Democratic                               1195 non-null int64\n",
      "Republican                               1195 non-null int64\n",
      "Party                                    1195 non-null int64\n",
      "dtypes: float64(11), int64(6), object(2)\n",
      "memory usage: 177.5+ KB\n"
     ]
    }
   ],
   "source": [
    "merged_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Partition data into training and validation sets using holdout method\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(merged_train[['FIPS', 'Total Population', 'Percent White, not Hispanic or Latino',\n",
    "                                                                 'Percent Black, not Hispanic or Latino', 'Percent Hispanic or Latino', 'Percent Foreign Born', \n",
    "                                                                 'Percent Female', 'Percent Age 29 and Under', 'Percent Age 65 and Older', 'Median Household Income', \n",
    "                                                                 'Percent Unemployed', 'Percent Less than High School Degree','Percent Less than Bachelor\\'s Degree', \n",
    "                                                                 'Percent Rural']], merged_train[['Democratic', 'Republican', 'Party']], train_size = .80, test_size = .20, \n",
    "                                                    random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 956 entries, 998 to 1061\n",
      "Data columns (total 14 columns):\n",
      "FIPS                                     956 non-null int64\n",
      "Total Population                         956 non-null int64\n",
      "Percent White, not Hispanic or Latino    956 non-null float64\n",
      "Percent Black, not Hispanic or Latino    956 non-null float64\n",
      "Percent Hispanic or Latino               956 non-null float64\n",
      "Percent Foreign Born                     956 non-null float64\n",
      "Percent Female                           956 non-null float64\n",
      "Percent Age 29 and Under                 956 non-null float64\n",
      "Percent Age 65 and Older                 956 non-null float64\n",
      "Median Household Income                  956 non-null int64\n",
      "Percent Unemployed                       956 non-null float64\n",
      "Percent Less than High School Degree     956 non-null float64\n",
      "Percent Less than Bachelor's Degree      956 non-null float64\n",
      "Percent Rural                            956 non-null float64\n",
      "dtypes: float64(11), int64(3)\n",
      "memory usage: 112.0 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(X_train.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 239 entries, 49 to 775\n",
      "Data columns (total 14 columns):\n",
      "FIPS                                     239 non-null int64\n",
      "Total Population                         239 non-null int64\n",
      "Percent White, not Hispanic or Latino    239 non-null float64\n",
      "Percent Black, not Hispanic or Latino    239 non-null float64\n",
      "Percent Hispanic or Latino               239 non-null float64\n",
      "Percent Foreign Born                     239 non-null float64\n",
      "Percent Female                           239 non-null float64\n",
      "Percent Age 29 and Under                 239 non-null float64\n",
      "Percent Age 65 and Older                 239 non-null float64\n",
      "Median Household Income                  239 non-null int64\n",
      "Percent Unemployed                       239 non-null float64\n",
      "Percent Less than High School Degree     239 non-null float64\n",
      "Percent Less than Bachelor's Degree      239 non-null float64\n",
      "Percent Rural                            239 non-null float64\n",
      "dtypes: float64(11), int64(3)\n",
      "memory usage: 28.0 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(X_val.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 956 entries, 998 to 1061\n",
      "Data columns (total 3 columns):\n",
      "Democratic    956 non-null int64\n",
      "Republican    956 non-null int64\n",
      "Party         956 non-null int64\n",
      "dtypes: int64(3)\n",
      "memory usage: 29.9 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(Y_train.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardizing training and validation set\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building multiple regression model to predict the number of votes cast for the Democratic party"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared for training set:   0.8756562459653711\n",
      "R-squared for validation set: 0.9082371310230191\n",
      "Adjusted R-squared for training set:   0.8738062857565668\n",
      "Adjusted R-squared for validation set: 0.9025019517119578\n"
     ]
    }
   ],
   "source": [
    "#Using all predictors\n",
    "model = linear_model.LinearRegression()\n",
    "fitted_model1 = model.fit(X_train_scaled, Y_train['Democratic'])\n",
    "\n",
    "#Computing coefficient of determination(R squared) for training and validation sets \n",
    "score_train1 = fitted_model1.score(X_train_scaled, Y_train['Democratic'])\n",
    "score_val1 = fitted_model1.score(X_val_scaled, Y_val['Democratic'])\n",
    "\n",
    "#Computing Adjusted R2 for training and validation set\n",
    "adj_r_squared_train1 = 1 - (1-score_train1)*(len(Y_train)-1)/(len(Y_train) - X_train_scaled.shape[1]-1)\n",
    "adj_r_squared_val1 = 1 - (1-score_val1)*(len(Y_val)-1)/(len(Y_val) - X_val_scaled.shape[1]-1)\n",
    "\n",
    "print(\"R-squared for training set:   \" + str(score_train1))\n",
    "print(\"R-squared for validation set: \" + str(score_val1))\n",
    "\n",
    "print(\"Adjusted R-squared for training set:   \" + str(adj_r_squared_train1)) \n",
    "print(\"Adjusted R-squared for validation set: \" + str(adj_r_squared_val1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.54760491e+03  6.40817239e+04 -3.30030267e+03 -6.97907957e+02\n",
      " -6.68236638e+03  2.69080813e+03 -4.36596380e+01 -3.96219001e+03\n",
      " -1.33771152e+03  3.74973714e+02  3.80786417e+02  3.96400877e+03\n",
      " -9.46588294e+03  6.47200629e+02]\n"
     ]
    }
   ],
   "source": [
    "#Use LASSO regression to see which predictors can be dropped.\n",
    "model = linear_model.Lasso(alpha = 1)\n",
    "fitted_model = model.fit(X_train_scaled, Y_train['Democratic'])\n",
    "print(fitted_model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.54792939e+03  6.39700378e+04 -3.25911869e+03 -6.55839853e+02\n",
      " -6.64529446e+03  2.75940852e+03 -3.13496241e+01 -3.96031733e+03\n",
      " -1.34851403e+03  3.76894577e+02  3.96267077e+02  3.90509300e+03\n",
      " -9.43590744e+03  6.36404514e+02]\n"
     ]
    }
   ],
   "source": [
    "#Use Ridge regression to see which predictors can be dropped.\n",
    "#Ridge regression alleviates multicollinearity among predictor variables by shrinking \n",
    "#the coeffiecient estimates of highly correlated variables.\n",
    "model = linear_model.Ridge(alpha = 1)\n",
    "fitted_model = model.fit(X_train_scaled, Y_train['Democratic'])\n",
    "print(fitted_model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared for training set:            0.8756345703184664\n",
      "Adjusted R-squared for training set:   0.8737842876239483\n",
      "\n",
      "R-squared for validation set:          0.9081562860355087\n",
      "Adjusted R-squared for validation set: 0.902416053912728\n"
     ]
    }
   ],
   "source": [
    "#Using different set of predictors to predict the number of votes cast for the Democratic party\n",
    "#Dropping indices [6,9,10] since the coefficients of those indices are close to 0.\n",
    "model = linear_model.LinearRegression()\n",
    "predictor_indices = [0,1,2,3,4,5,7,8,11,12,13]\n",
    "fitted_model = model.fit(X_train_scaled[:,predictor_indices], Y_train['Democratic'])\n",
    "\n",
    "#Computing coefficient of determination(R squared) for training and validation sets \n",
    "score_train = fitted_model.score(X_train_scaled[:,predictor_indices], Y_train['Democratic'])\n",
    "score_val = fitted_model.score(X_val_scaled[:,predictor_indices], Y_val['Democratic'])\n",
    "\n",
    "#Computing Adjusted R2 for training and validation set\n",
    "adj_r_squared_train = 1 - (1-score_train)*(len(Y_train)-1)/(len(Y_train) - X_train_scaled.shape[1]-1)\n",
    "adj_r_squared_val = 1 - (1-score_val)*(len(Y_val)-1)/(len(Y_val) - X_val_scaled.shape[1]-1)\n",
    "\n",
    "print(\"R-squared for training set:            \" + str(score_train))\n",
    "print(\"Adjusted R-squared for training set:   \" + str(adj_r_squared_train) + \"\\n\")  \n",
    "\n",
    "print(\"R-squared for validation set:          \" + str(score_val))\n",
    "print(\"Adjusted R-squared for validation set: \" + str(adj_r_squared_val)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared for training set:            0.8756105955553672\n",
      "Adjusted R-squared for training set:   0.8737599561693685\n",
      "\n",
      "R-squared for validation set:          0.9084590395414786\n",
      "Adjusted R-squared for validation set: 0.9027377295128209\n"
     ]
    }
   ],
   "source": [
    "#Using different set of predictors to predict the number of votes cast for the Democratic party\n",
    "#Dropping indices [3,6,9] \n",
    "model = linear_model.LinearRegression()\n",
    "predictor_indices = [0,1,2,4,5,7,8,10,11,12,13]\n",
    "fitted_model = model.fit(X_train_scaled[:,predictor_indices], Y_train['Democratic'])\n",
    "\n",
    "#Computing coefficient of determination(R squared) for training and validation sets \n",
    "score_train = fitted_model.score(X_train_scaled[:,predictor_indices], Y_train['Democratic'])\n",
    "score_val = fitted_model.score(X_val_scaled[:,predictor_indices], Y_val['Democratic'])\n",
    "\n",
    "#Computing Adjusted R2 for training and validation set\n",
    "adj_r_squared_train = 1 - (1-score_train)*(len(Y_train)-1)/(len(Y_train) - X_train_scaled.shape[1]-1)\n",
    "adj_r_squared_val = 1 - (1-score_val)*(len(Y_val)-1)/(len(Y_val) - X_val_scaled.shape[1]-1)\n",
    "\n",
    "print(\"R-squared for training set:            \" + str(score_train))\n",
    "print(\"Adjusted R-squared for training set:   \" + str(adj_r_squared_train) + \"\\n\")  \n",
    "\n",
    "print(\"R-squared for validation set:          \" + str(score_val))\n",
    "print(\"Adjusted R-squared for validation set: \" + str(adj_r_squared_val)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I think we should use dropping [3,6,9] since it gives best R-sq and Adj R-sq. \n",
    "#Let me know what you guys think...\n",
    "\n",
    "#Also I was trying to compute RMSE(Root mean squared error), but it was giving me errors. \n",
    "#I am not sure on how to compute that. We can use that as an evaluation metric too if one of us gets it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building multiple regression model to predict the number of votes cast for the Republican party"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared for training set:   0.8601764376210925\n",
      "R-squared for validation set: 0.8254428012168579\n",
      "Adjusted R-squared for training set:   0.8580961720809175\n",
      "Adjusted R-squared for validation set: 0.8145329762929114\n"
     ]
    }
   ],
   "source": [
    "#Using all predictors\n",
    "model = linear_model.LinearRegression()\n",
    "fitted_model1 = model.fit(X_train_scaled, Y_train['Republican'])\n",
    "\n",
    "#Computing coefficient of determination(R squared) for training and validation sets \n",
    "score_train1 = fitted_model1.score(X_train_scaled, Y_train['Republican'])\n",
    "score_val1 = fitted_model1.score(X_val_scaled, Y_val['Republican'])\n",
    "\n",
    "#Computing Adjusted R2 for training and validation set\n",
    "adj_r_squared_train1 = 1 - (1-score_train1)*(len(Y_train)-1)/(len(Y_train) - X_train_scaled.shape[1]-1)\n",
    "adj_r_squared_val1 = 1 - (1-score_val1)*(len(Y_val)-1)/(len(Y_val) - X_val_scaled.shape[1]-1)\n",
    "\n",
    "print(\"R-squared for training set:   \" + str(score_train1))\n",
    "print(\"R-squared for validation set: \" + str(score_val1))\n",
    "\n",
    "print(\"Adjusted R-squared for training set:   \" + str(adj_r_squared_train1)) \n",
    "print(\"Adjusted R-squared for validation set: \" + str(adj_r_squared_val1)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "[-1290.48554449 38921.35525714  2196.61930338 -2276.51228376\n",
      "  1493.99490852 -4977.64255795  -427.60976417  -893.68476944\n",
      "  2042.89323804  5069.09739533  1898.46206667  3236.46147907\n",
      " -3090.95256194 -5660.69521432]\n"
     ]
    }
   ],
   "source": [
    "#Use LASSO regression to see which predictors can be dropped. \n",
    "model = linear_model.Lasso(alpha = 1)\n",
    "fitted_model1 = model.fit(X_train_scaled, Y_train['Republican'])\n",
    "print(\"Training set:\")\n",
    "print(fitted_model1.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1289.74804593 38846.24890181  2191.49110254 -2263.51594138\n",
      "  1482.39884286 -4913.77723259  -418.0155558   -912.43755396\n",
      "  2012.73641618  5046.84743085  1899.93794145  3197.76676013\n",
      " -3085.01492839 -5655.57014574]\n"
     ]
    }
   ],
   "source": [
    "#Use Ridge regression to see which predictors can be dropped.\n",
    "#Ridge regression alleviates multicollinearity among predictor variables by shrinking \n",
    "#the coeffiecient estimates of highly correlated variables.\n",
    "model = linear_model.Ridge(alpha = 1)\n",
    "fitted_model = model.fit(X_train_scaled, Y_train['Republican'])\n",
    "print(fitted_model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared for training set:            0.859969795576287\n",
      "Adjusted R-squared for training set:   0.8578864556592499\n",
      "\n",
      "R-squared for validation set:          0.8253072209420018\n",
      "Adjusted R-squared for validation set: 0.8143889222508769\n"
     ]
    }
   ],
   "source": [
    "#Using different set of predictors to predict the number of votes cast for the Republican party\n",
    "#Dropping indices [6,7] since the coefficients of those indices are close to 0.\n",
    "model = linear_model.LinearRegression()\n",
    "predictor_indices = [0,1,2,3,4,5,8,9,10,11,12,13]\n",
    "fitted_model2 = model.fit(X_train_scaled[:,predictor_indices], Y_train['Republican'])\n",
    "\n",
    "#Computing coefficient of determination(R squared) for training and validation sets \n",
    "score_train2 = fitted_model2.score(X_train_scaled[:,predictor_indices], Y_train['Republican'])\n",
    "score_val2 = fitted_model2.score(X_val_scaled[:,predictor_indices], Y_val['Republican'])\n",
    "\n",
    "#Computing Adjusted R2 for training and validation set\n",
    "adj_r_squared_train2 = 1 - (1-score_train2)*(len(Y_train)-1)/(len(Y_train) - X_train_scaled.shape[1]-1)\n",
    "adj_r_squared_val2 = 1 - (1-score_val2)*(len(Y_val)-1)/(len(Y_val) - X_val_scaled.shape[1]-1)\n",
    "\n",
    "print(\"R-squared for training set:            \" + str(score_train2))\n",
    "print(\"Adjusted R-squared for training set:   \" + str(adj_r_squared_train2) + \"\\n\")  \n",
    "\n",
    "print(\"R-squared for validation set:          \" + str(score_val2))\n",
    "print(\"Adjusted R-squared for validation set: \" + str(adj_r_squared_val2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared for training set:            0.8576555694047268\n",
      "Adjusted R-squared for training set:   0.8555377989176558\n",
      "\n",
      "R-squared for validation set:          0.8269780594187519\n",
      "Adjusted R-squared for validation set: 0.816164188132424\n"
     ]
    }
   ],
   "source": [
    "#Using different set of predictors to predict the number of votes cast for the Republican party\n",
    "#Dropping indices [3,7,10]\n",
    "model = linear_model.LinearRegression()\n",
    "predictor_indices = [0,1,2,4,5,6,8,9,11,12,13]\n",
    "fitted_model2 = model.fit(X_train_scaled[:,predictor_indices], Y_train['Republican'])\n",
    "\n",
    "#Computing coefficient of determination(R squared) for training and validation sets \n",
    "score_train2 = fitted_model2.score(X_train_scaled[:,predictor_indices], Y_train['Republican'])\n",
    "score_val2 = fitted_model2.score(X_val_scaled[:,predictor_indices], Y_val['Republican'])\n",
    "\n",
    "#Computing Adjusted R2 for training and validation set\n",
    "adj_r_squared_train2 = 1 - (1-score_train2)*(len(Y_train)-1)/(len(Y_train) - X_train_scaled.shape[1]-1)\n",
    "adj_r_squared_val2 = 1 - (1-score_val2)*(len(Y_val)-1)/(len(Y_val) - X_val_scaled.shape[1]-1)\n",
    "\n",
    "print(\"R-squared for training set:            \" + str(score_train2))\n",
    "print(\"Adjusted R-squared for training set:   \" + str(adj_r_squared_train2) + \"\\n\")  \n",
    "\n",
    "print(\"R-squared for validation set:          \" + str(score_val2))\n",
    "print(\"Adjusted R-squared for validation set: \" + str(adj_r_squared_val2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I think we should use the one with dropping [6,7] just coz I get those indices from regression techniques.\n",
    "#Let me know what u guys think..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best summed precision: 1.7184873949579833 \n",
      "From columns: [0, 14] \n",
      "With split: [0.71848739 1.        ] \n",
      "And jump: 10\n",
      "\n",
      "Best summed recall: 1.435844513243894 \n",
      "From columns: [2, 14] \n",
      "With split: [0.90643275 0.52941176] \n",
      "And jump: 5\n",
      "\n",
      "Best tot f1_score: 1.465921787709497 \n",
      "From columns: [2, 14] \n",
      "With split: [0.86592179 0.6       ] \n",
      "And jump: 5\n"
     ]
    }
   ],
   "source": [
    "#isolate what we are interested in, which is the party (0/1)\n",
    "classifier = GaussianNB()  \n",
    "\n",
    "y4_train = Y_train['Party']\n",
    "y4_val = Y_val['Party']\n",
    "\n",
    "min_cols = 0\n",
    "max_cols = len(X_train.columns)\n",
    "\n",
    "max_precision = -100\n",
    "best_precision_cols = []\n",
    "best_precision_split = []\n",
    "best_precision_jump = -100\n",
    "\n",
    "max_recall = -100\n",
    "best_recall_cols = []\n",
    "best_recall_split = []\n",
    "best_recall_jump = -100\n",
    "\n",
    "max_f1_score = -100\n",
    "best_f1_split = []\n",
    "best_cols = []\n",
    "best_f1_jump = -100\n",
    "\n",
    "# #for every possible set of columns, get their evaluation metrics\n",
    "jump = 13\n",
    "for start_col in range(min_cols, max_cols+1): #from 0 to 15 in range\n",
    "    for end_col in range(start_col+1, max_cols+1,):#from from 1 to 15 in range\n",
    "        while jump != 0: #we test intervals of variables too\n",
    "            x4_train_scaled = X_train_scaled[:,start_col:end_col:jump]\n",
    "            x4_val_scaled = X_val_scaled[:,start_col:end_col:jump]\n",
    "            classifier.fit(x4_train_scaled, y4_train)\n",
    "            y_pred = classifier.predict(x4_val_scaled)\n",
    "\n",
    "            #get some evaluation metrics for the classifier\n",
    "            accuracy = metrics.accuracy_score(y4_val, y_pred)\n",
    "            error = 1 - accuracy\n",
    "            precision = metrics.precision_score(y4_val, y_pred, average = None)\n",
    "            recall = metrics.recall_score(y4_val, y_pred, average = None)\n",
    "            F1_score = metrics.f1_score(y4_val, y_pred, average = None )\n",
    "\n",
    "            if max_precision <= sum(precision): #returns the last instance of best prec\n",
    "                max_precision = sum(precision)\n",
    "                best_precision_split = precision\n",
    "                best_precision_jump = jump                \n",
    "                best_precision_cols = [start_col, end_col]          \n",
    "\n",
    "            if max_recall <= sum(recall): #returns the last instance of best recall\n",
    "                max_recall = sum(recall)\n",
    "                best_recall_split = recall\n",
    "                best_recall_jump = jump\n",
    "                best_recall_cols = [start_col, end_col]     \n",
    "\n",
    "            if max_f1_score <= sum(F1_score): #returns the last instance of best f1 score\n",
    "                max_f1_score = sum(F1_score)\n",
    "                best_f1_split = F1_score\n",
    "                best_f1_jump = jump\n",
    "                best_cols = [start_col, end_col]\n",
    "            jump = jump - 1\n",
    "        jump = 13\n",
    "\n",
    "#         print(\"\\nStart and end cols:\", [start_col, end_col],\n",
    "#               \"\\nJump is:\", jump,\n",
    "#               \"\\nAccuracy:\", accuracy,\n",
    "#               \"\\nError:\", error,\n",
    "#               \"\\nPrecision:\", precision,\n",
    "#               \"\\nRecall:\", recall,\n",
    "#               \"\\nF1:\", F1_score\n",
    "#              )\n",
    "\n",
    "print(\"\\nBest summed precision:\",max_precision, \n",
    "      \"\\nFrom columns:\", best_precision_cols,\n",
    "      \"\\nWith split:\", best_precision_split,\n",
    "      \"\\nAnd jump:\", best_precision_jump      \n",
    "     )\n",
    "print(\"\\nBest summed recall:\",max_recall, \n",
    "      \"\\nFrom columns:\", best_recall_cols,\n",
    "      \"\\nWith split:\", best_recall_split,\n",
    "      \"\\nAnd jump:\", best_recall_jump\n",
    "     )\n",
    "print(\"\\nBest tot f1_score:\", max_f1_score,\n",
    "      \"\\nFrom columns:\",best_cols,\n",
    "      \"\\nWith split:\",best_f1_split,\n",
    "      \"\\nAnd jump:\", best_f1_jump\n",
    "     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best summed precision: 1.7109181141439205 \n",
      "Using neighbors = 4 \n",
      "From columns: [0, 12] \n",
      "With split: [0.80769231 0.90322581] \n",
      "And jump: 1\n",
      "\n",
      "Best summed recall: 1.5596835225318197 \n",
      "Using neighbors = 1 \n",
      "From columns: [0, 6] \n",
      "With split: [0.85380117 0.70588235] \n",
      "And jump: 2\n",
      "\n",
      "Best tot f1_score: 1.568867281166817 \n",
      "Using neighbors = 1 \n",
      "From columns: [0, 3] \n",
      "With split: [0.88184438 0.6870229 ] \n",
      "And jump: 1\n"
     ]
    }
   ],
   "source": [
    "#isolate what we are interested in, which is the party (0/1)\n",
    "\n",
    "y4_train = Y_train['Party']\n",
    "y4_val = Y_val['Party']\n",
    "\n",
    "min_cols = 0\n",
    "max_cols = len(X_train.columns)\n",
    "\n",
    "max_precision = -100\n",
    "best_precision_cols = []\n",
    "best_precision_split = []\n",
    "best_precision_neighbors = -100\n",
    "best_precision_jump = -100\n",
    "\n",
    "max_recall = -100\n",
    "best_recall_cols = []\n",
    "best_recall_split = []\n",
    "best_recall_neighbors = -100\n",
    "best_recall_jump = -100\n",
    "\n",
    "max_f1_score = -100\n",
    "best_f1_split = []\n",
    "best_cols = []\n",
    "best_f1_neighbors = -100\n",
    "best_f1_jump = -100\n",
    "\n",
    "jump = 13\n",
    "#for every possible set of columns and neighbors 1-5, get their evaluation metrics\n",
    "for start_col in range(min_cols, max_cols+1): #from 0 to 15 in range\n",
    "    for end_col in range(start_col+1, max_cols+1):#from from 1 to 15 in range\n",
    "        while jump != 0: #we test intervals of variables too        \n",
    "            x4_train_scaled = X_train_scaled[:,start_col:end_col:jump]\n",
    "            x4_val_scaled = X_val_scaled[:,start_col:end_col:jump]\n",
    "\n",
    "            #for number of neighbors from 1 to 5\n",
    "            for neighbors in range(1,6):\n",
    "#                 print(start_col, end_col, jump, neighbors)\n",
    "                classifier = KNeighborsClassifier(n_neighbors = neighbors)  \n",
    "                classifier.fit(x4_train_scaled, y4_train)\n",
    "\n",
    "                y_pred = classifier.predict(x4_val_scaled)\n",
    "\n",
    "                #get some evaluation metrics for the classifier\n",
    "                accuracy = metrics.accuracy_score(y4_val, y_pred)\n",
    "                error = 1 - accuracy\n",
    "                precision = metrics.precision_score(y4_val, y_pred, average = None)\n",
    "                recall = metrics.recall_score(y4_val, y_pred, average = None)\n",
    "                F1_score = metrics.f1_score(y4_val, y_pred, average = None )\n",
    "\n",
    "                if max_precision <= sum(precision): #returns the last instance of best prec\n",
    "                    max_precision = sum(precision)\n",
    "                    best_precision_split = precision\n",
    "                    best_precision_neighbors = neighbors\n",
    "                    best_precision_jump = jump\n",
    "                    best_precision_cols = [start_col, end_col]          \n",
    "\n",
    "                if max_recall <= sum(recall): #returns the last instance of best recall\n",
    "                    max_recall = sum(recall)\n",
    "                    best_recall_split = recall\n",
    "                    best_recall_neighbors = neighbors     \n",
    "                    best_recall_jump = jump                    \n",
    "                    best_recall_cols = [start_col, end_col]     \n",
    "\n",
    "                if max_f1_score <= sum(F1_score): #returns the last instance of best f1 score\n",
    "                    max_f1_score = sum(F1_score)\n",
    "                    best_f1_split = F1_score\n",
    "                    best_f1_neighbors = neighbors\n",
    "                    best_f1_jump = jump\n",
    "                    best_cols = [start_col, end_col]\n",
    "\n",
    "    #             print(\"\\nStart and end cols:\", [start_col, end_col],\n",
    "    #                   \"\\nAccuracy:\", accuracy,\n",
    "    #                   \"\\nError:\", error,\n",
    "    #                   \"\\nPrecision:\", precision,\n",
    "    #                   \"\\nRecall:\", recall,\n",
    "    #                   \"\\nF1:\", F1_score\n",
    "    #                  )\n",
    "            jump = jump - 1\n",
    "        jump = 13\n",
    "\n",
    "print(\"\\nBest summed precision:\",max_precision, \n",
    "      \"\\nUsing neighbors =\", best_precision_neighbors,\n",
    "      \"\\nFrom columns:\", best_precision_cols,\n",
    "      \"\\nWith split:\", best_precision_split,\n",
    "      \"\\nAnd jump:\", best_precision_jump     \n",
    "     )\n",
    "print(\"\\nBest summed recall:\",max_recall, \n",
    "      \"\\nUsing neighbors =\", best_recall_neighbors,      \n",
    "      \"\\nFrom columns:\", best_recall_cols,\n",
    "      \"\\nWith split:\", best_recall_split,\n",
    "      \"\\nAnd jump:\", best_recall_jump      \n",
    "     )\n",
    "print(\"\\nBest tot f1_score:\", max_f1_score,\n",
    "      \"\\nUsing neighbors =\", best_f1_neighbors,      \n",
    "      \"\\nFrom columns:\",best_cols,\n",
    "      \"\\nWith split:\",best_f1_split,\n",
    "      \"\\nAnd jump:\", best_f1_jump\n",
    "     )            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTING, IGNORE THIS PART\n",
    "# X_train_scaled[1:3,0:14:10]\n",
    "# X_train_scaled[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the models we observed, k-nearest neighbors, using FIPS, Total Population, % White, and % Hispanic, while having k=1, provided us with the best f1 score. We used f1 score as our main measurement for deciding which model had the best performance since it accounts for both the precision and recall. Additionally, we had a sum of the f1 score to find that which provided a maximal return on deciding both 0's and 1's in our Party. The performance of our best model has an f1 score of 88.2% on 0's and 68.7% on 1's.\n",
    "\n",
    "The parameters of our naive bayes model were default parameters. This is because we have no prior probabilities and do not account for variance smoothing, which are the two main parameters available to us. For our variables, we enumerated through all possible combinations and selected the combination that gave us the best f1 score. \n",
    "\n",
    "The parameters of our k-nearest neighbors was switching the values of k from 1-5. No changes were made to the distance parameter, so Minkowski with p=2 (Euclidean) was kept from default. For our variables, we enumerated through all possible combinations and selected the combination that gave us the best f1 score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler  \n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from sklearn.cluster import KMeans, DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>County</th>\n",
       "      <th>FIPS</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Percent White, not Hispanic or Latino</th>\n",
       "      <th>Percent Black, not Hispanic or Latino</th>\n",
       "      <th>Percent Hispanic or Latino</th>\n",
       "      <th>Percent Foreign Born</th>\n",
       "      <th>Percent Female</th>\n",
       "      <th>Percent Age 29 and Under</th>\n",
       "      <th>Percent Age 65 and Older</th>\n",
       "      <th>Median Household Income</th>\n",
       "      <th>Percent Unemployed</th>\n",
       "      <th>Percent Less than High School Degree</th>\n",
       "      <th>Percent Less than Bachelor's Degree</th>\n",
       "      <th>Percent Rural</th>\n",
       "      <th>Democratic</th>\n",
       "      <th>Republican</th>\n",
       "      <th>Party</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AZ</td>\n",
       "      <td>apache</td>\n",
       "      <td>4001</td>\n",
       "      <td>72346</td>\n",
       "      <td>18.571863</td>\n",
       "      <td>0.486551</td>\n",
       "      <td>5.947806</td>\n",
       "      <td>1.719515</td>\n",
       "      <td>50.598513</td>\n",
       "      <td>45.854643</td>\n",
       "      <td>13.322091</td>\n",
       "      <td>32460</td>\n",
       "      <td>15.807433</td>\n",
       "      <td>21.758252</td>\n",
       "      <td>88.941063</td>\n",
       "      <td>74.061076</td>\n",
       "      <td>16298</td>\n",
       "      <td>7810</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AZ</td>\n",
       "      <td>cochise</td>\n",
       "      <td>4003</td>\n",
       "      <td>128177</td>\n",
       "      <td>56.299492</td>\n",
       "      <td>3.714395</td>\n",
       "      <td>34.403208</td>\n",
       "      <td>11.458374</td>\n",
       "      <td>49.069646</td>\n",
       "      <td>37.902276</td>\n",
       "      <td>19.756275</td>\n",
       "      <td>45383</td>\n",
       "      <td>8.567108</td>\n",
       "      <td>13.409171</td>\n",
       "      <td>76.837055</td>\n",
       "      <td>36.301067</td>\n",
       "      <td>17383</td>\n",
       "      <td>26929</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AZ</td>\n",
       "      <td>coconino</td>\n",
       "      <td>4005</td>\n",
       "      <td>138064</td>\n",
       "      <td>54.619597</td>\n",
       "      <td>1.342855</td>\n",
       "      <td>13.711033</td>\n",
       "      <td>4.825298</td>\n",
       "      <td>50.581614</td>\n",
       "      <td>48.946141</td>\n",
       "      <td>10.873943</td>\n",
       "      <td>51106</td>\n",
       "      <td>8.238305</td>\n",
       "      <td>11.085381</td>\n",
       "      <td>65.791439</td>\n",
       "      <td>31.466066</td>\n",
       "      <td>34240</td>\n",
       "      <td>19249</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AZ</td>\n",
       "      <td>gila</td>\n",
       "      <td>4007</td>\n",
       "      <td>53179</td>\n",
       "      <td>63.222325</td>\n",
       "      <td>0.552850</td>\n",
       "      <td>18.548675</td>\n",
       "      <td>4.249798</td>\n",
       "      <td>50.296170</td>\n",
       "      <td>32.238290</td>\n",
       "      <td>26.397638</td>\n",
       "      <td>40593</td>\n",
       "      <td>12.129932</td>\n",
       "      <td>15.729958</td>\n",
       "      <td>82.262624</td>\n",
       "      <td>41.062000</td>\n",
       "      <td>7643</td>\n",
       "      <td>12180</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AZ</td>\n",
       "      <td>graham</td>\n",
       "      <td>4009</td>\n",
       "      <td>37529</td>\n",
       "      <td>51.461536</td>\n",
       "      <td>1.811932</td>\n",
       "      <td>32.097844</td>\n",
       "      <td>4.385942</td>\n",
       "      <td>46.313518</td>\n",
       "      <td>46.393456</td>\n",
       "      <td>12.315809</td>\n",
       "      <td>47422</td>\n",
       "      <td>14.424104</td>\n",
       "      <td>14.580797</td>\n",
       "      <td>86.675944</td>\n",
       "      <td>46.437399</td>\n",
       "      <td>3368</td>\n",
       "      <td>6870</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  State    County  FIPS  Total Population  \\\n",
       "0    AZ    apache  4001             72346   \n",
       "1    AZ   cochise  4003            128177   \n",
       "2    AZ  coconino  4005            138064   \n",
       "3    AZ      gila  4007             53179   \n",
       "4    AZ    graham  4009             37529   \n",
       "\n",
       "   Percent White, not Hispanic or Latino  \\\n",
       "0                              18.571863   \n",
       "1                              56.299492   \n",
       "2                              54.619597   \n",
       "3                              63.222325   \n",
       "4                              51.461536   \n",
       "\n",
       "   Percent Black, not Hispanic or Latino  Percent Hispanic or Latino  \\\n",
       "0                               0.486551                    5.947806   \n",
       "1                               3.714395                   34.403208   \n",
       "2                               1.342855                   13.711033   \n",
       "3                               0.552850                   18.548675   \n",
       "4                               1.811932                   32.097844   \n",
       "\n",
       "   Percent Foreign Born  Percent Female  Percent Age 29 and Under  \\\n",
       "0              1.719515       50.598513                 45.854643   \n",
       "1             11.458374       49.069646                 37.902276   \n",
       "2              4.825298       50.581614                 48.946141   \n",
       "3              4.249798       50.296170                 32.238290   \n",
       "4              4.385942       46.313518                 46.393456   \n",
       "\n",
       "   Percent Age 65 and Older  Median Household Income  Percent Unemployed  \\\n",
       "0                 13.322091                    32460           15.807433   \n",
       "1                 19.756275                    45383            8.567108   \n",
       "2                 10.873943                    51106            8.238305   \n",
       "3                 26.397638                    40593           12.129932   \n",
       "4                 12.315809                    47422           14.424104   \n",
       "\n",
       "   Percent Less than High School Degree  Percent Less than Bachelor's Degree  \\\n",
       "0                             21.758252                            88.941063   \n",
       "1                             13.409171                            76.837055   \n",
       "2                             11.085381                            65.791439   \n",
       "3                             15.729958                            82.262624   \n",
       "4                             14.580797                            86.675944   \n",
       "\n",
       "   Percent Rural  Democratic  Republican  Party  \n",
       "0      74.061076       16298        7810      1  \n",
       "1      36.301067       17383       26929      0  \n",
       "2      31.466066       34240       19249      1  \n",
       "3      41.062000        7643       12180      0  \n",
       "4      46.437399        3368        6870      0  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardizing training and validation set\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#we work off the whole dataset, so we do not look at the already partitioned data\n",
    "Y_set = merged_train[['Democratic','Republican','Party','FIPS']] #what we are looking to predict\n",
    "Y_set_party = Y_set['Party']\n",
    "\n",
    "X_set = merged_train.drop(Y_set, axis=1) #remove the val set from test set\n",
    "X_set = X_set.drop(['State','County'], axis=1) #remove non-numeric values\n",
    "\n",
    "scaler.fit(X_set)\n",
    "t6_scaled = scaler.transform(X_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = ['single','complete','average','ward']\n",
    "\n",
    "min_cols = 0\n",
    "max_cols = len(X_set.columns)\n",
    "\n",
    "best_rand = -100\n",
    "best_rand_set = []\n",
    "best_rand_jump = -999\n",
    "best_rand_method = ''\n",
    "best_rand_silh = -100\n",
    "\n",
    "best_silh = -100\n",
    "best_silh_set = []\n",
    "best_silh_jump = -999\n",
    "best_silh_method = ''\n",
    "best_silh_rand = -100\n",
    "\n",
    "best_tot_score = -100\n",
    "best_tot_set = []\n",
    "best_tot_jump = -999\n",
    "best_tot_method = ''\n",
    "best_tot_sections = []\n",
    "\n",
    "jump = 1\n",
    "#for every possible set of columns a\n",
    "for start_col in range(min_cols, max_cols+1): #from 1 to 15 in range\n",
    "    for end_col in range(start_col+1, max_cols+1):#from from 2 to 15 in range\n",
    "        while jump != end_col:\n",
    "               \n",
    "            t6_scaled_iter = t6_scaled[:,start_col:end_col:jump] #set the columns we will use \n",
    "        \n",
    "            #applying our clustering techniques\n",
    "            for method in methods: #make changes to each criteria\n",
    "                print([start_col, end_col, jump, method])         #RUN THIS TO SEE IT GO...                \n",
    "                clustering = linkage(t6_scaled_iter, method = method, metric = \"euclidean\")\n",
    "                clusters = fcluster(clustering, 2, criterion = \"maxclust\")\n",
    "\n",
    "                #supervised evaluation metric\n",
    "                adjusted_rand_index = metrics.adjusted_rand_score(Y_set_party, clusters)\n",
    "                \n",
    "                #unsupervised evaluation metric\n",
    "                silhouette_coefficient = metrics.silhouette_score(t6_scaled_iter, clusters, metric = \"euclidean\")\n",
    "                \n",
    "                if adjusted_rand_index >= best_rand:\n",
    "                    best_rand = adjusted_rand_index\n",
    "                    best_rand_set = [start_col,end_col]\n",
    "                    best_rand_jump = jump\n",
    "                    best_rand_method = method\n",
    "                    best_rand_silh = silhouette_coefficient\n",
    "                    \n",
    "                if silhouette_coefficient >= best_silh:\n",
    "                    best_silh = silhouette_coefficient\n",
    "                    best_silh_set = [start_col, end_col]\n",
    "                    best_silh_jump = jump\n",
    "                    best_silh_method = method\n",
    "                    best_silh_rand = adjusted_rand_index\n",
    "                    \n",
    "                curr_tot_score = adjusted_rand_index + silhouette_coefficient\n",
    "        \n",
    "                if curr_tot_score >= best_tot_score:\n",
    "                    best_tot_score = curr_tot_score\n",
    "                    best_tot_set = [start_col, end_col]\n",
    "                    best_tot_jump = jump\n",
    "                    best_tot_method = method\n",
    "                    best_tot_sections = [adjusted_rand_index, silhouette_coefficient]\n",
    "                    \n",
    "#                 print([adjusted_rand_index, silhouette_coefficient])\n",
    "#                 print(\"Start and end cols:\", [start_col, end_col],\n",
    "#                       \"\\nJump:\", jump,\n",
    "#                       \"\\nMethod for linkage:\", method,\n",
    "#                       \"\\nAdjusted rand_index:\", adjusted_rand_index,\n",
    "#                       \"\\nSilhouette coeff:\", silhouette_coefficient\n",
    "#                      )\n",
    "\n",
    "            jump = jump + 1\n",
    "        jump = 1\n",
    "        \n",
    "#we want a high adjusted rand index, as close to to 1 as possible\n",
    "print(\"\\nHighest adjusted rand index:\", best_rand,\n",
    "      \"\\nFrom start and end cols:\", best_rand_set,\n",
    "      \"\\nWith jump:\", best_rand_jump,\n",
    "      \"\\nUsing method:\", best_rand_method,\n",
    "      \"\\nWith silhouette:\", best_rand_silh\n",
    "     )\n",
    "#we want a high silhouette coefficient, as close to 1 as possible\n",
    "print(\"\\nHighest silhouette coefficient:\", best_silh,\n",
    "      \"\\nFrom start and end cols:\", best_silh_set,\n",
    "      \"\\nWith jump:\", best_silh_jump,\n",
    "      \"\\nUsing method:\", best_silh_method,\n",
    "      \"\\nWith rand index:\", best_silh_rand\n",
    "     )\n",
    "\n",
    "#we want the highest max double number\n",
    "print(\"\\nHighest total score:\", best_tot_score,\n",
    "      \"\\nFrom start and end cols:\", best_tot_set,\n",
    "      \"\\nWith jump:\", best_tot_jump,\n",
    "      \"\\nUsing method:\", best_tot_method,\n",
    "      \"\\nWith index and coefficient:\", best_tot_sections\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inits = ['k-means++','random']\n",
    "\n",
    "min_cols = 0\n",
    "max_cols = len(X_set.columns)\n",
    "\n",
    "best_rand = -100\n",
    "best_rand_set = []\n",
    "best_rand_jump = -999\n",
    "best_rand_method = ''\n",
    "best_rand_silh = -100\n",
    "\n",
    "best_silh = -100\n",
    "best_silh_set = []\n",
    "best_silh_jump = -999\n",
    "best_silh_method = ''\n",
    "best_silh_rand = -100\n",
    "\n",
    "best_tot_score = -100\n",
    "best_tot_set = []\n",
    "best_tot_jump = -999\n",
    "best_tot_method = ''\n",
    "best_tot_sections = []\n",
    "\n",
    "jump = 13\n",
    "#for every possible set of columns a\n",
    "for start_col in range(min_cols, max_cols+1): #from 1 to 15 in range\n",
    "    for end_col in range(start_col+1, max_cols+1):#from from 2 to 15 in range\n",
    "        while jump != 0:\n",
    "            t6_scaled_iter = t6_scaled[:,start_col:end_col:jump] #set the columns we will use \n",
    "        \n",
    "            #applying our clustering techniques\n",
    "            for the_init in inits: #make changes to each criteria\n",
    "                #for time brevity, we choose n_init as 5\n",
    "                #we should ideally have n_clusters = 2, one for dem, one for rep\n",
    "                clustering = KMeans(n_clusters = 2,\n",
    "                                    init = the_init, \n",
    "                                    random_state = 0, \n",
    "                                    n_init = 5).fit(t6_scaled\n",
    "                                    )\n",
    "                clusters = clustering.labels_\n",
    "\n",
    "                #supervised evaluation metric\n",
    "                adjusted_rand_index = metrics.adjusted_rand_score(Y_set_party, clusters)\n",
    "                \n",
    "                #unsupervised evaluation metric\n",
    "                silhouette_coefficient = metrics.silhouette_score(t6_scaled_iter, clusters, metric = \"euclidean\")\n",
    "                \n",
    "#                 print([start_col, end_col, jump, method])\n",
    "                \n",
    "                if adjusted_rand_index >= best_rand:\n",
    "                    best_rand = adjusted_rand_index\n",
    "                    best_rand_set = [start_col,end_col]\n",
    "                    best_rand_jump = jump\n",
    "                    best_rand_method = the_init\n",
    "                    best_rand_silh = silhouette_coefficient\n",
    "                    \n",
    "                if silhouette_coefficient >= best_silh:\n",
    "                    best_silh = silhouette_coefficient\n",
    "                    best_silh_set = [start_col, end_col]\n",
    "                    best_silh_jump = jump\n",
    "                    best_silh_method = the_init\n",
    "                    best_silh_rand = adjusted_rand_index\n",
    "                    \n",
    "                curr_tot_score = adjusted_rand_index + silhouette_coefficient\n",
    "        \n",
    "                if curr_tot_score >= best_tot_score:\n",
    "                    best_tot_score = curr_tot_score\n",
    "                    best_tot_set = [start_col, end_col]\n",
    "                    best_tot_jump = jump\n",
    "                    best_tot_method = the_init\n",
    "                    best_tot_sections = [adjusted_rand_index, silhouette_coefficient]\n",
    "                    \n",
    "#                 print([adjusted_rand_index, silhouette_coefficient])\n",
    "#                 print(\"Start and end cols:\", [start_col, end_col],\n",
    "#                       \"\\nJump:\", jump,\n",
    "#                       \"\\nMethod for linkage:\", method,\n",
    "#                       \"\\nAdjusted rand_index:\", adjusted_rand_index,\n",
    "#                       \"\\nSilhouette coeff:\", silhouette_coefficient\n",
    "#                      )\n",
    "\n",
    "            jump = jump - 1\n",
    "        jump = 13\n",
    "        \n",
    "#we want a high adjusted rand index, as close to to 1 as possible\n",
    "print(\"\\nHighest adjusted rand index:\", best_rand,\n",
    "      \"\\nFrom start and end cols:\", best_rand_set,\n",
    "      \"\\nWith jump:\", best_rand_jump,\n",
    "      \"\\nUsing initialization:\", best_rand_method,\n",
    "      \"\\nWith silhouette:\", best_rand_silh\n",
    "     )\n",
    "#we want a high silhouette coefficient, as close to 1 as possible\n",
    "print(\"\\nHighest silhouette coefficient:\", best_silh,\n",
    "      \"\\nFrom start and end cols:\", best_silh_set,\n",
    "      \"\\nWith jump:\", best_silh_jump,\n",
    "      \"\\nUsing initialization:\", best_silh_method,\n",
    "      \"\\nWith rand index:\", best_silh_rand\n",
    "     )\n",
    "\n",
    "#we want the highest max double number\n",
    "print(\"\\nHighest total score:\", best_tot_score,\n",
    "      \"\\nFrom start and end cols:\", best_tot_set,\n",
    "      \"\\nWith jump:\", best_tot_jump,\n",
    "      \"\\nUsing initialization:\", best_tot_method,\n",
    "      \"\\nWith index and coefficient:\", best_tot_sections\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two clustering methods involved were hierarchical clustering, and k-means clustering. From testing the two methods with several models, we decided that a summation of both the adjusted rand index and silhouette coefficient would show the best overall performance. \n",
    "\n",
    "The hierchical clustering using Ward's method, provided the best performance. This was also chosen using columns 1 and 14. This provided an exceptionally high silhouette coefficient, which is a very good indication in terms of unsupervised evaluation. \n",
    "\n",
    "It should be noted, if we were opting for a higher rand index, which shows how close we are to the ground truth, Ward's also provides a higher value. We have also provided the highest possible rand index in both sets of models, and it is by using Ward's method, giving us a value of 0.2787940184428421. Therefore, we note that Ward's hierarchical clustering method is the best for clustering that we have observed.\n",
    "\n",
    "For variables, we enumerated through all possible sets of columns and provided the column sets and their highest results.\n",
    "\n",
    "For parameters, in hierarchical clustering we tested 4 different linkage models of single, complete, ward's, and average. We found that Ward's was the best performing of the four. In k-means, kept a static n_init and n_clusters. n_init was kept static to keep the model from taking too long to run, and n_init was set to 2 since we reasoned that there should be only two clusters for the two parties. We modified the initialization method, which would determine the beginning of our centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
